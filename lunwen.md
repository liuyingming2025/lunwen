# Let's Verify Step by Step 论文总结

## 1. 引言

大型语言模型已能够通过生成一步步的思维链（chain-of-thought）来解决需要复杂多步推理的任务。然而，即使是最先进的模型也容易产生谬误——它们在不确定时会捏造事实（虚构事实）。这些"幻觉"（hallucinations）在需要多步推理的领域中尤其成问题，因为一个逻辑错误就足以使整个解决方案失效。

研究人员开发了训练奖励模型（reward models）来区分理想和非理想输出的有效方法。奖励模型可用于强化学习流程或通过拒绝采样进行搜索。然而，这些系统的可靠性取决于奖励模型本身的可靠性，因此研究如何最有效地训练可靠的奖励模型至关重要。

相关工作中，Uesato等人（2022）描述了训练奖励模型的两种不同方法：
- **结果监督**（outcome supervision）：仅使用模型思维链的最终结果进行训练
- **过程监督**（process supervision）：为思维链中的每一步提供反馈

过程监督具有多种优势：
1. 提供更精确的反馈，指出错误的确切位置
2. 更容易被人类解释
3. 更直接地奖励模型遵循人类认可的思维链

尽管有这些优势，Uesato等人发现在小学数学领域中，结果监督和过程监督导致的最终性能相似。

本论文的主要贡献：
1. 证明过程监督能够训练比结果监督更可靠的奖励模型，使用最先进的PRM（过程监督奖励模型）解决了MATH测试集代表性子集的78.2%的问题
2. 证明大型奖励模型可以可靠地近似人类监督，可用于高效进行大规模数据收集实验
3. 证明主动学习可将过程监督的数据效率提高2.6倍
4. 发布完整的过程监督数据集PRM800K，以促进相关研究

## 2. 方法

研究者进行了结果监督和过程监督的比较，遵循与Uesato等人（2022）类似的方法。MATH数据集中的所有问题都有自动可检查的答案，因此结果监督可以在没有人类参与的情况下提供。相比之下，过程监督需要人类数据标注员具体标记模型生成解决方案中每一步的正确性。

研究在两个不同的规模进行了实验：大规模和小规模。在大规模实验中，所有模型都从GPT-4微调而来，目标是通过训练最可靠的ORM（结果监督奖励模型）和PRM（过程监督奖励模型）来推进最先进技术。在小规模实验中，为了进行更直接的比较，使用大规模模型来监督小规模模型训练，这种设置使得能够进行一些重要的消融实验。

### 2.1 范围

在每个模型规模中，使用单一固定模型（称为生成器）生成所有解决方案。本研究没有尝试通过强化学习改进生成器，而是专注于如何训练最可靠的奖励模型。通过奖励模型在生成器均匀采样的解决方案上执行best-of-N搜索来评估奖励模型，对于每个测试问题，选择奖励模型排名最高的解决方案，根据其最终答案自动评分。

### 2.2 基础模型

所有大规模模型都是从基础GPT-4模型微调而来。小规模基础模型在设计上与GPT-4相似，但预训练计算量减少了约200倍。作为额外的预训练步骤，所有模型都在一个包含约15亿个数学相关标记的数据集（称为MathMix）上进行了微调，这提高了模型的数学推理能力。

### 2.3 生成器

为了让解析个别步骤更容易，生成器训练为以换行符分隔的逐步格式生成解决方案。具体来说，通过少样本生成MATH训练问题的解决方案，过滤得到达到正确最终答案的解决方案，并在此数据集上微调基础模型一个周期。这一步不是为了教给生成器新技能，而是为了教会生成器以所需格式生成解决方案。

### 2.4 数据收集

为收集过程监督数据，向人类数据标注员展示由大规模生成器采样的MATH问题的逐步解决方案。标注员的任务是为解决方案的每一步分配正面、负面或中性标签。

整个步骤级标签数据集称为PRM800K，其训练集包含12K个问题的75K个解决方案中的800K个步骤级标签。为最小化过拟合，PRM800K训练集包含了4.5K个MATH测试问题的数据，因此只在剩余的500个MATH测试问题上评估模型。

在数据收集过程中，研究者采用策略性选择向数据标注员展示哪些解决方案，特别是选择展示"令人信服的错误答案解决方案"。"令人信服"指的是当前最佳PRM高度评价的解决方案，"错误答案"指的是达到不正确最终答案的解决方案。

### 2.5 结果监督奖励模型（ORMs）

ORM训练方法类似于Cobbe等人（2021）的方法。从生成器均匀采样固定数量的每个问题的解决方案，训练ORM预测每个解决方案是正确还是不正确。在测试时，使用ORM在最终标记处的预测作为解决方案的总体分数。

### 2.6 过程监督奖励模型（PRMs）

PRM训练为预测每一步后最后一个标记处该步骤的正确性。在测试时，通过对整个解决方案执行单次PRM前向传递来确定步骤级预测。为比较多个解决方案，需要为每个解决方案计算单一分数，将解决方案的PRM分数定义为PRM下每一步都正确的概率，实现为每一步正确性概率的乘积。

在提供过程监督时，故意选择仅监督到第一个不正确步骤。这使得结果监督和过程监督的比较更加直接。对于正确的解决方案，两种方法提供相同的信息（即每一步都是正确的）。对于不正确的解决方案，两种方法都揭示了至少一个错误的存在，而过程监督还揭示了该错误的确切位置。

## 3. 大规模监督

使用PRM800K中的步骤级标签训练大规模PRM。为确保大规模ORM基线尽可能强大，在生成器的每个问题的100个均匀样本上进行训练，这意味着ORM训练集与PRM800K没有重叠，并且规模大一个数量级。

结果显示，虽然ORM的表现略优于多数投票基线，但PRM明显优于两者。PRM不仅在所有N值下都达到更高的性能，而且随着N的增加，性能差距也在扩大。这表明PRM在搜索大量模型生成的解决方案时比ORM和多数投票更有效。

## 4. 小规模合成监督

为了更好地比较结果和过程监督，研究者使用大规模PRM监督小型模型。这种设置使得能够以适中的成本模拟大量数据收集。

### 4.1 过程vs结果监督

在直接比较中，对于每个数据集，提供三种形式的监督：来自大型PRM的过程监督、来自大型PRM的结果监督和来自最终答案检查的结果监督。结果显示，过程监督在所有数据收集规模上都明显优于两种形式的结果监督。使用大型PRM进行结果监督比最终答案检查更有效，这可以解释为大型PRM为使用不正确推理达到正确最终答案的解决方案提供了更好的监督。

### 4.2 主动学习

研究还调查了主动学习的影响。训练一个小规模奖励模型PRM_selector，用于从每个问题中对大量样本进行评分。在选择训练样本时，80%是最令人信服的错误答案样本，20%是剩余最令人信服的样本。结果表明，这种形式的主动学习比均匀数据标记的数据效率大约高2.6倍。

## 5. 分布外泛化

为了评估分布外泛化能力，在224个STEM问题的保留集上评估了大规模ORM和PRM，这些问题来自最近的AP物理、AP微积分、AP化学、AMC10和AMC12考试。结果与第3节中的结果相似：PRM优于ORM和多数投票。这表明PRM可以容忍适度的分布偏移，其强大性能在新测试问题上仍然保持。

## 6. 讨论

### 6.1 信用分配

过程监督的一个明显优势是它提供比结果监督更精确的反馈。用结果监督训练的奖励模型面临困难的信用分配任务——为了很好地泛化，它必须确定不正确的解决方案在哪里出错。相比之下，过程监督提供更丰富的信号：它既指定了实际上有多少第一步是正确的，也指定了不正确步骤的精确位置。

### 6.2 对齐影响

过程监督在AI对齐方面比结果监督有几个优势：
1. 更可能产生可解释的推理，因为它鼓励模型遵循人类认可的思维过程
2. 本质上更安全：直接奖励对齐的思维链，而不是依赖结果作为对齐行为的代理
3. 不会产生"对齐税"（alignment tax），即在对齐和性能之间的权衡

### 6.3 测试集污染

MATH数据集的测试集包含在多个在线场所讨论的问题，这些问题可能出现在模型的预训练数据集中。尽管尝试使用字符串匹配启发式方法从MathMix数据集中删除所有MATH问题，但很难对MathMix和MATH数据集之间的重叠做出强有力的保证。

## 7. 相关工作

### 7.1 结果vs过程监督

在密切相关的工作中，Uesato等人（2022）比较了小学数学领域中结果和过程监督的影响。他们发现两种方法导致类似的最终答案错误率，过程监督以更少的数据实现了这些结果。然而，本文的数据缩放趋势表明，少量过程监督和大量结果监督确实会导致类似的性能，这与Uesato等人的结果一致。趋势还表明，即使仅基于结果进行判断，规模扩大的过程监督也优于结果监督。

### 7.2 合成监督

类似于第4节的工作，Gao等人（2022）使用大型奖励模型监督小型模型的训练。他们研究了RLHF期间发生的过度优化，这些实验需要大量人类偏好数据。为解决这一挑战，他们使用金标准奖励模型代替人类反馈。

### 7.3 自然语言推理

最近几项关于大型语言模型推理能力的研究与本文隐含相关。Lewkowycz等人（2022）表明，在大量技术内容上微调模型能显著提高MATH上的性能。Wang等人（2022）表明，自洽性在许多推理基准上表现极为强大，且不需要额外的微调。Wei等人（2022）和Nye等人（2021）证明了通过思维链或草稿显式执行中间推理步骤的重要性。Kojima等人（2022）表明，模型能够零样本执行这种行为，仅依赖简单提示。

## 8. 结论

研究证明，在数学推理领域，过程监督可用于训练比结果监督更可靠的奖励模型。还表明，主动学习可用于通过仅向人类反馈显示最有价值的模型完成来降低人类数据收集的成本。研究发布了PRM800K，即用于训练最先进奖励模型的完整人类反馈数据集，希望消除这一重要的入门障碍，以催化关于大型语言模型对齐的相关研究。
